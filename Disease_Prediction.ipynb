{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0VLsPrMP6Ol",
        "outputId": "121a03af-7ba3-4549-f3a7-a3be2694b5bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Loading data from: /content/drive/My Drive/Major Project (Medical System)/Symptoms_Illness_Prediction_Dataset.csv\n",
            "Data shape: (4961, 133)\n",
            "Detected target column: prognosis\n",
            "Duplicates before removal: 4657\n",
            "Data shape after removing duplicates: (304, 133)\n",
            "Final feature matrix shape: (304, 132)\n",
            "Classes (first 20): ['AIDS', 'Acne', 'Alcoholic Hepatitis', 'Allergy', 'Arthritis', 'Bronchial Asthma', 'Cervical Spondylosis', 'Chickenpox', 'Chronic Cholestasis', 'Common Cold', 'Dengue', 'Diabetes', 'Dimorphic Hemmorhoids (piles)', 'Drug Reaction', 'Fungal Infection', 'GERD', 'Gastroenteritis', 'Heart Attack', 'Hepatitis A', 'Hepatitis B']\n",
            "Train size: (243, 132), Test size: (61, 132)\n",
            "\n",
            "--- Evaluating Backpropagation ---\n",
            "Backpropagation test accuracy: 0.9836  |  macro-F1: 0.9707\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         1\n",
            "           1       1.00      1.00      1.00         1\n",
            "           2       1.00      1.00      1.00         2\n",
            "           3       1.00      1.00      1.00         1\n",
            "           4       1.00      1.00      1.00         1\n",
            "           5       1.00      1.00      1.00         1\n",
            "           6       1.00      1.00      1.00         1\n",
            "           7       1.00      1.00      1.00         2\n",
            "           8       0.67      1.00      0.80         2\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       1.00      1.00      1.00         2\n",
            "          11       1.00      1.00      1.00         2\n",
            "          12       1.00      1.00      1.00         1\n",
            "          13       1.00      1.00      1.00         1\n",
            "          14       1.00      1.00      1.00         1\n",
            "          15       1.00      1.00      1.00         1\n",
            "          16       1.00      1.00      1.00         1\n",
            "          17       1.00      1.00      1.00         1\n",
            "          18       1.00      1.00      1.00         2\n",
            "          19       1.00      1.00      1.00         2\n",
            "          20       0.00      0.00      0.00         1\n",
            "          21       1.00      1.00      1.00         2\n",
            "          22       1.00      1.00      1.00         2\n",
            "          23       1.00      1.00      1.00         1\n",
            "          24       1.00      1.00      1.00         2\n",
            "          25       1.00      1.00      1.00         2\n",
            "          26       1.00      1.00      1.00         2\n",
            "          27       1.00      1.00      1.00         1\n",
            "          28       1.00      1.00      1.00         2\n",
            "          29       1.00      1.00      1.00         2\n",
            "          30       1.00      1.00      1.00         2\n",
            "          31       1.00      1.00      1.00         1\n",
            "          32       1.00      1.00      1.00         1\n",
            "          33       1.00      1.00      1.00         1\n",
            "          34       1.00      1.00      1.00         2\n",
            "          35       1.00      1.00      1.00         1\n",
            "          36       1.00      1.00      1.00         2\n",
            "          37       1.00      1.00      1.00         2\n",
            "          38       1.00      1.00      1.00         1\n",
            "          39       1.00      1.00      1.00         2\n",
            "          40       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.98        61\n",
            "   macro avg       0.97      0.98      0.97        61\n",
            "weighted avg       0.97      0.98      0.98        61\n",
            "\n",
            "\n",
            "Best model on test set: Backpropagation with accuracy 0.9836\n",
            "\n",
            "Cross-Validation Evaluation with Backpropagation:\n",
            "Cross-validation accuracy scores: [1.         0.98360656 1.         1.         1.        ]\n",
            "Mean CV accuracy: 0.9967 | Std: 0.0066\n",
            "Saved artifacts to /content/drive/My Drive/Major Project (Medical System)/Symptoms_Illness_Prediction_Output\n",
            "Example prediction call (zeros): {'Itching': 0, 'Skin rash': 0, 'Nodal skin eruptions': 0, 'Continuous sneezing': 0}\n",
            "Predicted (example): Heart Attack\n",
            "Top 5 probabilities (example):\n",
            "  Heart Attack: 0.22\n",
            "  Chronic Cholestasis: 0.07\n",
            "  Cervical Spondylosis: 0.05\n",
            "  Urinary Tract Infection: 0.04\n",
            "  Fungal Infection: 0.04\n",
            "All artifacts saved. Ready for downstream integration.\n",
            "\n",
            "Sample Test 1:\n",
            "Predicted disease: Chronic Cholestasis\n",
            "Top 5 predicted diseases with probabilities:\n",
            "  Chronic Cholestasis: 0.23\n",
            "  Jaundice: 0.13\n",
            "  Heart Attack: 0.10\n",
            "  Impetigo: 0.07\n",
            "  Chickenpox: 0.06\n",
            "\n",
            "Sample Test 2:\n",
            "Predicted disease: Jaundice\n",
            "Top 5 predicted diseases with probabilities:\n",
            "  Jaundice: 0.56\n",
            "  Hypothyroidism: 0.08\n",
            "  Hypertension: 0.07\n",
            "  Allergy: 0.04\n",
            "  Hyperthyroidism: 0.03\n",
            "\n",
            "Sample Test 3:\n",
            "Predicted disease: Cervical Spondylosis\n",
            "Top 5 predicted diseases with probabilities:\n",
            "  Cervical Spondylosis: 0.25\n",
            "  Heart Attack: 0.17\n",
            "  Paralysis (brain hemorrhage): 0.17\n",
            "  Hypertension: 0.11\n",
            "  AIDS: 0.04\n",
            "\n",
            "Sample Test 4:\n",
            "Predicted disease: GERD\n",
            "Top 5 predicted diseases with probabilities:\n",
            "  GERD: 0.81\n",
            "  Peptic Ulcer Disease: 0.03\n",
            "  Chronic Cholestasis: 0.02\n",
            "  Heart Attack: 0.01\n",
            "  Hepatitis D: 0.01\n",
            "\n",
            "Sample Test 5:\n",
            "Predicted disease: Arthritis\n",
            "Top 5 predicted diseases with probabilities:\n",
            "  Arthritis: 0.66\n",
            "  Hypothyroidism: 0.04\n",
            "  Hyperthyroidism: 0.03\n",
            "  Paralysis (brain hemorrhage): 0.03\n",
            "  Heart Attack: 0.03\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from google.colab import drive\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neural_network import MLPClassifier   # Backpropagation (Neural Net)\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_curve, auc, confusion_matrix\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Dataset Path\n",
        "file_path = \"/content/drive/My Drive/Major Project (Medical System)/Symptoms_Illness_Prediction_Dataset.csv\"\n",
        "# Set output directory where artifacts will be saved\n",
        "output_dir = \"/content/drive/My Drive/Major Project (Medical System)/Symptoms_Illness_Prediction_Output\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Settings\n",
        "do_grid_search = False   # Grid search for tuning\n",
        "random_state = 42\n",
        "test_size = 0.20\n",
        "cv_folds = 5\n",
        "\n",
        "\n",
        "# ---------------- Helper Functions ----------------\n",
        "\n",
        "def load_data(path):\n",
        "    df = pd.read_csv(path)\n",
        "    return df\n",
        "\n",
        "def detect_target(df):\n",
        "    for c in df.columns:\n",
        "        if str(c).strip().lower() == \"prognosis\":\n",
        "            return c\n",
        "    return df.columns[-1]\n",
        "\n",
        "def simplify_label(name):\n",
        "    s = str(name)\n",
        "    s = s.replace(\"_\", \" \").replace(\"-\", \" \").replace(\".\", \" \").strip()\n",
        "    s = \" \".join(s.split())\n",
        "    return s.lower().capitalize()\n",
        "\n",
        "def preprocess_features(df, target_col):\n",
        "    nrows = len(df)\n",
        "    drop_cols = [c for c in df.columns if df[c].nunique() == nrows]\n",
        "    if drop_cols:\n",
        "        print(\"Dropping unique columns:\", drop_cols)\n",
        "        df = df.drop(columns=drop_cols)\n",
        "    X = df.drop(columns=[target_col]).copy()\n",
        "    y = df[target_col].astype(str).str.strip().copy()\n",
        "    for c in X.columns:\n",
        "        if X[c].dtype == object:\n",
        "            vals = set(map(str.lower, X[c].dropna().astype(str).unique()))\n",
        "            if vals <= {\"yes\",\"no\",\"true\",\"false\",\"0\",\"1\"}:\n",
        "                X[c] = X[c].astype(str).str.lower().map(\n",
        "                    {\"yes\":1,\"true\":1,\"1\":1,\"no\":0,\"false\":0,\"0\":0}).astype(float)\n",
        "    non_numeric = X.select_dtypes(include=[\"object\",\"category\"]).columns.tolist()\n",
        "    if non_numeric:\n",
        "        print(\"One-hot encoding:\", non_numeric)\n",
        "        X = pd.get_dummies(X, columns=non_numeric, drop_first=True)\n",
        "    X = X.fillna(X.median())\n",
        "    return X, y\n",
        "\n",
        "def save_artifacts(best_estimator, label_encoder, feature_mapping, out_dir):\n",
        "    joblib.dump(best_estimator, os.path.join(out_dir, \"best_pipeline.joblib\"))\n",
        "    joblib.dump(label_encoder, os.path.join(out_dir, \"label_encoder.joblib\"))\n",
        "    with open(os.path.join(out_dir, \"feature_mapping.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(feature_mapping, f, indent=2, ensure_ascii=False)\n",
        "    print(\"Saved artifacts to\", out_dir)\n",
        "\n",
        "def predict_from_simplified_input(simplified_dict, feature_mapping, pipeline, label_encoder):\n",
        "    features = list(feature_mapping.keys())\n",
        "    X_in = pd.DataFrame([0]*len(features)).T\n",
        "    X_in.columns = features\n",
        "\n",
        "    inverse_map = {v.lower():k for k,v in feature_mapping.items()}\n",
        "\n",
        "    for k,v in simplified_dict.items():\n",
        "        key = k\n",
        "        if key.lower() in inverse_map:\n",
        "            orig = inverse_map[key.lower()]\n",
        "        elif key in feature_mapping:\n",
        "            orig = key\n",
        "        else:\n",
        "            norm = key.replace(\"_\",\" \").strip().lower()\n",
        "            orig = inverse_map.get(norm, None)\n",
        "        if orig is None:\n",
        "            print(f\"Warning: input key '{k}' not recognized. Ignored.\")\n",
        "            continue\n",
        "        X_in[orig] = float(v)\n",
        "    X_in = X_in.astype(float)[features]\n",
        "\n",
        "    probs = pipeline.predict_proba(X_in)[0]\n",
        "    idx = np.argmax(probs)\n",
        "    label = label_encoder.inverse_transform([idx])[0] if hasattr(label_encoder, \"inverse_transform\") else idx\n",
        "\n",
        "    top_idx = np.argsort(probs)[::-1][:5]\n",
        "    top = [(label_encoder.inverse_transform([i])[0], float(probs[i])) for i in top_idx]\n",
        "    return label, top, probs\n",
        "\n",
        "def build_preprocessing_pipeline():\n",
        "    return Pipeline([\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "def evaluate_models(X_train, y_train, X_test, y_test, perform_grid=False):\n",
        "    results = {}\n",
        "    models = {\n",
        "        \"Backpropagation\": MLPClassifier(\n",
        "            hidden_layer_sizes=(100,),\n",
        "            activation='relu',\n",
        "            solver='adam',\n",
        "            max_iter=500,\n",
        "            random_state=random_state)\n",
        "    }\n",
        "    param_grids = {\n",
        "        \"Backpropagation\": {\n",
        "            \"clf__hidden_layer_sizes\": [(50,), (100,), (100,50)],\n",
        "            \"clf__activation\": [\"relu\", \"tanh\"],\n",
        "            \"clf__solver\": [\"adam\", \"sgd\"]\n",
        "        }\n",
        "    }\n",
        "    prep = build_preprocessing_pipeline()\n",
        "    prep.fit(X_train)\n",
        "    for name, model in models.items():\n",
        "        print(\"\\n--- Evaluating\", name, \"---\")\n",
        "        pipeline = Pipeline([\n",
        "            (\"scaler\", prep.named_steps[\"scaler\"]),\n",
        "            (\"clf\", model)\n",
        "        ])\n",
        "        if perform_grid:\n",
        "            grid = GridSearchCV(pipeline, param_grids[name], cv=cv_folds, n_jobs=-1, verbose=2)\n",
        "            grid.fit(X_train, y_train)\n",
        "            best = grid.best_estimator_\n",
        "            print(\"Best params:\", grid.best_params_)\n",
        "            y_pred = best.predict(X_test)\n",
        "            results[name] = {\"estimator\": best, \"grid_search\": grid}\n",
        "        else:\n",
        "            pipeline.fit(X_train, y_train)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            results[name] = {\"estimator\": pipeline}\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "        print(f\"{name} test accuracy: {acc:.4f}  |  macro-F1: {f1:.4f}\")\n",
        "        print(\"Classification report:\")\n",
        "        print(classification_report(y_test, y_pred, zero_division=0))\n",
        "    return results\n",
        "\n",
        "def cross_validation_evaluation(X, y):\n",
        "    print(\"\\nCross-Validation Evaluation with Backpropagation:\")\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", MLPClassifier(hidden_layer_sizes=(100,), activation='relu',\n",
        "                              solver='adam', max_iter=500, random_state=random_state))\n",
        "    ])\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "    cv_scores = cross_val_score(pipeline, X, y, cv=skf, scoring=\"accuracy\", n_jobs=-1)\n",
        "    print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
        "    print(f\"Mean CV accuracy: {cv_scores.mean():.4f} | Std: {cv_scores.std():.4f}\")\n",
        "\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "\n",
        "def main():\n",
        "    print(\"Loading data from:\", file_path)\n",
        "    df = load_data(file_path)\n",
        "    print(\"Data shape:\", df.shape)\n",
        "    target_col = detect_target(df)\n",
        "    print(\"Detected target column:\", target_col)\n",
        "    print(f\"Duplicates before removal: {df.duplicated().sum()}\")\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Data shape after removing duplicates: {df.shape}\")\n",
        "\n",
        "    X, y = preprocess_features(df, target_col)\n",
        "    print(\"Final feature matrix shape:\", X.shape)\n",
        "    feature_mapping = {c: simplify_label(c) for c in X.columns}\n",
        "\n",
        "    summary = {\n",
        "        \"shape\": df.shape, \"n_features\": X.shape[1],\n",
        "        \"target_col\": target_col, \"n_classes\": int(y.nunique())\n",
        "    }\n",
        "    with open(os.path.join(output_dir, \"dataset_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "    with open(os.path.join(output_dir, \"feature_mapping.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(feature_mapping, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_enc = le.fit_transform(y)\n",
        "    print(\"Classes (first 20):\", list(le.classes_)[:20])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_enc, test_size=test_size, stratify=y_enc, random_state=random_state)\n",
        "    print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
        "\n",
        "    results = evaluate_models(X_train, y_train, X_test, y_test, perform_grid=do_grid_search)\n",
        "\n",
        "    best_name, best_score, best_estimator = None, -1, None\n",
        "    for name, info in results.items():\n",
        "        est = info[\"estimator\"]\n",
        "        y_pred = est.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        if acc > best_score:\n",
        "            best_score = acc\n",
        "            best_name = name\n",
        "            best_estimator = est\n",
        "\n",
        "    print(f\"\\nBest model on test set: {best_name} with accuracy {best_score:.4f}\")\n",
        "\n",
        "    cross_validation_evaluation(X, y_enc)\n",
        "\n",
        "    save_artifacts(best_estimator, le, feature_mapping, output_dir)\n",
        "\n",
        "    example_input = {}\n",
        "    if len(feature_mapping) > 0:\n",
        "        keys = list(feature_mapping.values())[:4]\n",
        "        for k in keys:\n",
        "            example_input[k] = 0\n",
        "    print(\"Example prediction call (zeros):\", example_input)\n",
        "    predicted_label, top5, _ = predict_from_simplified_input(example_input, feature_mapping, best_estimator, le)\n",
        "    print(\"Predicted (example):\", predicted_label)\n",
        "    print(\"Top 5 probabilities (example):\")\n",
        "    for disease, prob in top5:\n",
        "        print(f\"  {disease}: {prob:.2f}\")\n",
        "\n",
        "    print(\"All artifacts saved. Ready for downstream integration.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "# ---------------- Sample Predictions ----------------\n",
        "\n",
        "# Load saved artifacts\n",
        "best_estimator = joblib.load(\"/content/drive/MyDrive/Major Project (Medical System)/Symptoms_Illness_Prediction_Output/best_pipeline.joblib\")\n",
        "label_encoder = joblib.load(\"/content/drive/MyDrive/Major Project (Medical System)/Symptoms_Illness_Prediction_Output/label_encoder.joblib\")\n",
        "with open(\"/content/drive/MyDrive/Major Project (Medical System)/Symptoms_Illness_Prediction_Output/feature_mapping.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    feature_mapping = json.load(f)\n",
        "\n",
        "# Sample inputs\n",
        "sample_tests = [\n",
        "    {\"Itching\": 1, \"Skin rash\": 1, \"Nodal skin eruptions\": 0, \"Continuous sneezing\": 0, \"Shivering\": 0, \"High fever\": 1},\n",
        "    {\"Headache\": 1, \"Cold hands and feets\": 1, \"Mood swings\": 1, \"Weight loss\": 1, \"High fever\": 1, \"Fatigue\": 1},\n",
        "    {\"Chest pain\": 1, \"Palpitations\": 1, \"Breathlessness\": 1, \"Fast heart rate\": 1, \"Weakness in limbs\": 1, \"Swelling joints\": 0},\n",
        "    {\"Acidity\": 1, \"Stomach pain\": 1, \"Vomiting\": 1, \"Loss of appetite\": 1, \"Nausea\": 1, \"Diarrhoea\": 0},\n",
        "    {\"Muscle weakness\": 1, \"Back pain\": 1, \"Joint pain\": 1, \"Movement stiffness\": 1, \"Loss of balance\": 0, \"Headache\": 0}\n",
        "]\n",
        "\n",
        "# Run predictions\n",
        "for i, sample in enumerate(sample_tests):\n",
        "    print(f\"\\nSample Test {i+1}:\")\n",
        "    predicted_label, top5, _ = predict_from_simplified_input(sample, feature_mapping, best_estimator, label_encoder)\n",
        "    print(f\"Predicted disease: {predicted_label}\")\n",
        "    print(\"Top 5 predicted diseases with probabilities:\")\n",
        "    for disease, prob in top5:\n",
        "        print(f\"  {disease}: {prob:.2f}\")\n"
      ]
    }
  ]
}